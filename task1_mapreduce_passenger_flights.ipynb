{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "VfzjJJBsn8GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import math\n",
        "from collections import defaultdict\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import pandas as pd\n",
        "\n",
        "# === Multi-threaded MapReduce Simulation\n",
        "# This solution mimics the parallel nature of MapReduce using Python threads.\n",
        "\n",
        "# Map Phase: Processes a chunk of rows and maps passenger flights\n",
        "def map_chunk(chunk):\n",
        "    \"\"\"\n",
        "    Map function: Converts a list of rows into key-value pairs (PassengerID, 1)\n",
        "    \"\"\"\n",
        "    return [(row[0], 1) for row in chunk]\n",
        "\n",
        "def multithreaded_map(filepath, num_threads=4):\n",
        "    \"\"\"\n",
        "    Split the dataset into chunks and process them in parallel using threads\n",
        "    \"\"\"\n",
        "    with open(filepath, mode='r') as file:\n",
        "        reader = list(csv.reader(file))\n",
        "\n",
        "    # Divide data into chunks for parallel mapping\n",
        "    chunk_size = math.ceil(len(reader) / num_threads)\n",
        "    chunks = [reader[i:i + chunk_size] for i in range(0, len(reader), chunk_size)]\n",
        "\n",
        "    mapped_results = []\n",
        "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "        futures = executor.map(map_chunk, chunks)\n",
        "        for result in futures:\n",
        "            mapped_results.extend(result)\n",
        "\n",
        "    return mapped_results"
      ],
      "metadata": {
        "id": "LnRMvkmds8FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reduce Phase: Processes a chunk of mapped pairs\n",
        "def reduce_chunk(mapped_data):\n",
        "    \"\"\"\n",
        "    Reduce function: Aggregates counts for each PassengerID in a chunk\n",
        "    \"\"\"\n",
        "    local_counts = defaultdict(int)\n",
        "    for passenger_id, count in mapped_data:\n",
        "        local_counts[passenger_id] += count\n",
        "    return local_counts\n",
        "\n",
        "def merge_counts(partials):\n",
        "    \"\"\"\n",
        "    Merge function: Combines partial counts from different threads\n",
        "    \"\"\"\n",
        "    final_counts = defaultdict(int)\n",
        "    for partial in partials:\n",
        "        for pid, count in partial.items():\n",
        "            final_counts[pid] += count\n",
        "    return final_counts\n",
        "\n",
        "def multithreaded_reduce(mapped_data, num_threads=4):\n",
        "    \"\"\"\n",
        "    Run reduce step in parallel across chunks of mapped data\n",
        "    \"\"\"\n",
        "    chunk_size = math.ceil(len(mapped_data) / num_threads)\n",
        "    chunks = [mapped_data[i:i + chunk_size] for i in range(0, len(mapped_data), chunk_size)]\n",
        "\n",
        "    partial_results = []\n",
        "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "        futures = executor.map(reduce_chunk, chunks)\n",
        "        for result in futures:\n",
        "            partial_results.append(result)\n",
        "\n",
        "    return merge_counts(partial_results)"
      ],
      "metadata": {
        "id": "zIFCn-JQs-2m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}